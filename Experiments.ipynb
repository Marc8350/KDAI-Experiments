{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a17bfec9",
            "metadata": {},
            "source": [
                "# GoLLIE Experiment Runner for Colab\n",
                "\n",
                "This notebook provides a complete environment setup and experiment execution for the GoLLIE project. It includes:\n",
                "1. Environment configuration (Git, Dependencies)\n",
                "2. Data downloading and preparation\n",
                "3. Running evaluations across multiple guideline modules"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a607a2d1",
            "metadata": {},
            "source": [
                "## 1. Project Setup\n",
                "Detect environment, clone repositories, and install dependencies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8176cd04",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "\n",
                "# 1. Detect if we are in Colab and clone YOUR experiments repo if needed\n",
                "is_colab = 'google.colab' in sys.modules\n",
                "\n",
                "if is_colab:\n",
                "    print(\"Detected Google Colab environment.\")\n",
                "    # Clone your main experiment repo to get guidelines and requirements\n",
                "    REPO_URL = \"https://github.com/Marc8350/KDAI-Experiments.git\"\n",
                "    REPO_NAME = \"KDAI-Experiments\"\n",
                "    if not os.path.exists(REPO_NAME):\n",
                "        print(f\"Cloning {REPO_NAME}...\")\n",
                "        !git clone {REPO_URL}\n",
                "        %cd {REPO_NAME}\n",
                "    else:\n",
                "        %cd {REPO_NAME}\n",
                "else:\n",
                "    print(\"Running in local environment.\")\n",
                "\n",
                "# 3. Install requirements\n",
                "if os.path.exists(\"requirements.txt\"):\n",
                "    print(\"Installing dependencies (this may take a few minutes)...\")\n",
                "    %pip install -r requirements.txt\n",
                "    %pip install bitsandbytes accelerate # Required for quantization\n",
                "else:\n",
                "    print(\"requirements.txt not found!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6f095c53",
            "metadata": {},
            "source": [
                "## 2. Data Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ac0dc79a",
            "metadata": {},
            "outputs": [],
            "source": [
                "def download_few_nerd():\n",
                "    expected_test_dir = \"few-nerd_test\"\n",
                "    if os.path.isdir(expected_test_dir):\n",
                "        print(\"Dataset already prepared.\")\n",
                "        return\n",
                "\n",
                "    print(\"Downloading Few-NERD dataset...\")\n",
                "    from datasets import load_dataset\n",
                "    dataset = load_dataset(\"DFKI-SLT/few-nerd\", name='supervised')\n",
                "\n",
                "    for split_name, split_dataset in dataset.items():\n",
                "        save_path = f\"few-nerd_{split_name}\"\n",
                "        print(f\"Saving {split_name} split...\")\n",
                "        split_dataset.save_to_disk(save_path)\n",
                "    print(\"Dataset setup completed.\")\n",
                "\n",
                "download_few_nerd()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e22adec3",
            "metadata": {},
            "source": [
                "## 3. GoLLIE Experiments Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fa70e5cd",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "# --- CONFIGURATION ---\n",
                "TEST_MODE = True                # Set to False for a full experiment run\n",
                "TEST_MODULE_IDX = 0            # Select which guideline module to use (0-13)\n",
                "TEST_SENTENCE_IDX = 0           # Select which specific sentence to process\n",
                "\n",
                "# Colab Resource Management\n",
                "USE_4BIT = True                 # HIGHLY RECOMMENDED for free Colab T4 GPUs\n",
                "USE_FLASH_ATTN = False          # Use only on A100/L4 GPUs. Set False for T4.\n",
                "DTYPE = \"bfloat16\" if torch.cuda.is_bf16_supported() else \"float16\"\n",
                "# ---------------------"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fa745020",
            "metadata": {},
            "source": [
                "## 4. Experiment Logic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "23ce176f",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import json\n",
                "import re\n",
                "import inspect\n",
                "import logging\n",
                "import black\n",
                "from datetime import datetime\n",
                "from typing import Dict, List, Type, Any\n",
                "from datasets import load_from_disk\n",
                "from jinja2 import Template\n",
                "\n",
                "PROJECT_ROOT = os.getcwd()\n",
                "if PROJECT_ROOT not in sys.path: sys.path.insert(0, PROJECT_ROOT)\n",
                "\n",
                "GOLLIE_PATH = os.path.join(PROJECT_ROOT, \"GoLLIE\")\n",
                "if GOLLIE_PATH not in sys.path: sys.path.append(GOLLIE_PATH)\n",
                "\n",
                "from src.model.load_model import load_model\n",
                "from src.tasks.utils_typing import Entity, AnnotationList\n",
                "from src.tasks.utils_scorer import SpanScorer\n",
                "\n",
                "from annotation_guidelines import (\n",
                "    guidelines_coarse_gollie, guidelines_coarse_gollie_detailed_v1, guidelines_coarse_gollie_detailed_v2,\n",
                "    guidelines_coarse_gollie_detailed_v3, guidelines_coarse_gollie_v1, guidelines_coarse_gollie_v2,\n",
                "    guidelines_coarse_gollie_v3, guidelines_fine_gollie, guidelines_fine_gollie_detailed_v1,\n",
                "    guidelines_fine_gollie_detailed_v2, guidelines_fine_gollie_detailed_v3, guidelines_fine_gollie_v1,\n",
                "    guidelines_fine_gollie_v2, guidelines_fine_gollie_v3\n",
                ")\n",
                "\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "guideline_modules = [\n",
                "    guidelines_coarse_gollie, guidelines_coarse_gollie_detailed_v1, guidelines_coarse_gollie_detailed_v2,\n",
                "    guidelines_coarse_gollie_detailed_v3, guidelines_coarse_gollie_v1, guidelines_coarse_gollie_v2,\n",
                "    guidelines_coarse_gollie_v3, guidelines_fine_gollie, guidelines_fine_gollie_detailed_v1,\n",
                "    guidelines_fine_gollie_detailed_v2, guidelines_fine_gollie_detailed_v3, guidelines_fine_gollie_v1,\n",
                "    guidelines_fine_gollie_v2, guidelines_fine_gollie_v3\n",
                "]\n",
                "\n",
                "MODEL_LOAD_PARAMS = {\n",
                "    \"inference\": True,\n",
                "    \"model_weights_name_or_path\": \"HiTZ/GoLLIE-7B\",\n",
                "    \"quantization\": 4 if USE_4BIT else None,\n",
                "    \"use_lora\": False,\n",
                "    \"force_auto_device_map\": True,\n",
                "    \"use_flash_attention\": USE_FLASH_ATTN,\n",
                "    \"torch_dtype\": DTYPE\n",
                "}\n",
                "\n",
                "GENERATE_PARAMS = {\n",
                "    \"max_new_tokens\": 128,\n",
                "    \"do_sample\": False,\n",
                "    \"min_new_tokens\": 0,\n",
                "    \"num_beams\": 1,\n",
                "    \"num_return_sequences\": 1,\n",
                "}\n",
                "\n",
                "class MyEntityScorer(SpanScorer):\n",
                "    valid_types: List[Type] = []\n",
                "    def __call__(self, reference: List[List[Entity]], predictions: List[List[Entity]]) -> Dict[str, Any]:\n",
                "        output = super().__call__(reference, predictions)\n",
                "        return {\"entities\": output[\"spans\"]}\n",
                "\n",
                "def label_to_classname(label):\n",
                "    if label == \"O\": return None\n",
                "    parts = re.split(r'[-/]', label)\n",
                "    return \"\".join(p.capitalize() for p in parts)\n",
                "\n",
                "def run_experiment(test_mode=False, test_m_idx=0, test_s_idx=0):\n",
                "    RESULTS_DIR = \"GOLLIE-results\"\n",
                "    os.makedirs(RESULTS_DIR, exist_ok=True)\n",
                "    \n",
                "    ds = load_from_disk(\"./few-nerd_test\")\n",
                "    coarse_names = ds.features[\"ner_tags\"].feature.names\n",
                "    fine_names = ds.features[\"fine_ner_tags\"].feature.names\n",
                "\n",
                "    print(\"Loading model with params:\", MODEL_LOAD_PARAMS)\n",
                "    model, tokenizer = load_model(**MODEL_LOAD_PARAMS)\n",
                "\n",
                "    template_path = os.path.join(GOLLIE_PATH, \"templates\", \"prompt.txt\")\n",
                "    with open(template_path, \"rt\") as f:\n",
                "        template = Template(f.read())\n",
                "\n",
                "    active_modules = [guideline_modules[test_m_idx]] if test_mode else guideline_modules\n",
                "\n",
                "    for module in active_modules:\n",
                "        module_name = module.__name__\n",
                "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "        log_filename = os.path.join(RESULTS_DIR, f\"{module_name}_{timestamp}.json\")\n",
                "        \n",
                "        logging.info(f\"Processing module: {module_name}\")\n",
                "        tag_key = \"ner_tags\" if \"coarse\" in module_name else \"fine_ner_tags\"\n",
                "        names_ref = coarse_names if \"coarse\" in module_name else fine_names\n",
                "        \n",
                "        scorer = MyEntityScorer()\n",
                "        scorer.valid_types = module.ENTITY_DEFINITIONS\n",
                "        gold_per_module, predictions_per_module, sentence_results = [], [], []\n",
                "\n",
                "        if test_mode:\n",
                "            sentences_to_process = [(test_s_idx, ds[test_s_idx])]\n",
                "            print(f\"TEST MODE: Running index {test_s_idx}\")\n",
                "        else:\n",
                "            sentences_to_process = enumerate(ds)\n",
                "\n",
                "        for i, sentence in sentences_to_process:\n",
                "            tokens = sentence[\"tokens\"]\n",
                "            text = \" \".join(tokens)\n",
                "            tags = sentence[tag_key]\n",
                "            gold = []\n",
                "            for token, tag_id in zip(tokens, tags):\n",
                "                class_name = label_to_classname(names_ref[tag_id])\n",
                "                if class_name:\n",
                "                    entity_class = getattr(module, class_name, None)\n",
                "                    if entity_class: gold.append(entity_class(span=token))\n",
                "            \n",
                "            formatted_text = template.render(\n",
                "                guidelines=[inspect.getsource(def_obj) for def_obj in module.ENTITY_DEFINITIONS],\n",
                "                text=text, annotations=gold, gold=gold\n",
                "            )\n",
                "            \n",
                "            try: formatted_text = black.format_str(formatted_text, mode=black.Mode())\n",
                "            except: pass\n",
                "\n",
                "            prompt = formatted_text.split(\"result =\")[0] + \"result =\"\n",
                "            inputs = tokenizer(prompt, add_special_tokens=True, return_tensors=\"pt\")\n",
                "            inputs = {k: v[:, :-1].to(model.device) for k, v in inputs.items()}\n",
                "            \n",
                "            out = model.generate(**inputs, **GENERATE_PARAMS)\n",
                "            res_str = tokenizer.decode(out[0], skip_special_tokens=True).split(\"result =\")[-1]\n",
                "            \n",
                "            try: prediction = AnnotationList.from_output(res_str, task_module=module_name)\n",
                "            except: prediction = []\n",
                "\n",
                "            score = scorer(reference=[gold], predictions=[prediction])\n",
                "            gold_per_module.append(gold)\n",
                "            predictions_per_module.append(prediction)\n",
                "            \n",
                "            sentence_results.append({\n",
                "                \"index\": i, \"timestamp\": datetime.now().isoformat(), \"text\": text,\n",
                "                \"gold\": [str(g) for g in gold], \"prediction\": [str(p) for p in prediction], \"score\": score\n",
                "            })\n",
                "            \n",
                "            # 7. Intermediate Saving (Avoid data loss on long runs)\n",
                "            current_overall_score = scorer(reference=gold_per_module, predictions=predictions_per_module)\n",
                "            final_results = {\n",
                "                \"module\": module_name, \"timestamp\": timestamp, \"model_load_params\": MODEL_LOAD_PARAMS,\n",
                "                \"generate_params\": GENERATE_PARAMS, \"overall_score\": current_overall_score,\n",
                "                \"processed_count\": len(sentence_results), \"sentences\": sentence_results\n",
                "            }\n",
                "            \n",
                "            with open(log_filename, \"w\") as f: json.dump(final_results, f, indent=4)\n",
                "            if i % 10 == 0: logging.info(f\"[{module_name}] Progress: {i} sentences saved.\")\n",
                "\n",
                "    logging.info(\"Execution finished.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "12697550",
            "metadata": {},
            "source": [
                "## 5. Run Experiment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "74cd692d",
            "metadata": {},
            "outputs": [],
            "source": [
                "run_experiment(\n",
                "    test_mode=TEST_MODE, \n",
                "    test_m_idx=TEST_MODULE_IDX, \n",
                "    test_s_idx=TEST_SENTENCE_IDX\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
