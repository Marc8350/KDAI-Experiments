{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Run GoLLIE-7B on Google Colab (Free Tier)\n",
                "\n",
                "This notebook allows you to run the **HiTZ/GoLLIE-7B** model on Google Colab's free tier using 4-bit quantization.\n",
                "\n",
                "### Note on Flash Attention\n",
                "You correctly noted that GoLLIE defaults to Flash Attention. However, **Flash Attention 2** requires Ampere GPUs (A100), while Colab Free Tier usually provides T4 GPUs (Turing architecture). \n",
                "\n",
                "On a T4, we cannot use `use_flash_attention_2=True`. Instead, this notebook uses the default attention implementation (or `sdpa` - Scaled Dot Product Attention) which works perfectly fine on T4, just slightly less memory-efficient than Flash Attention 2. 4-bit quantization ensures it still fits easily within the 16GB VRAM.\n",
                "\n",
                "## Instructions\n",
                "1. **Runtime Type**: Ensure you are using a GPU runtime (Runtime > Change runtime type > T4 GPU).\n",
                "2. **Files**: Upload your generated `guidelines_coarse_gollie.py` or `guidelines_fine_gollie.py` files to the Colab file explorer (sidebar on the left).\n",
                "3. **Run All**: Execute the cells below in order."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Install Dependencies\n",
                "!pip install -q transformers accelerate bitsandbytes sentencepiece"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Setup Environment and Mock Imports\n",
                "# Since your guideline files import from 'src.tasks.utils_typing', we mock this module\n",
                "# so you don't need to clone the entire repository just to define the guidelines.\n",
                "\n",
                "import sys\n",
                "from types import ModuleType\n",
                "from dataclasses import dataclass\n",
                "\n",
                "# Create a mock module structure\n",
                "src = ModuleType(\"src\")\n",
                "src_tasks = ModuleType(\"src.tasks\")\n",
                "src_tasks_utils_typing = ModuleType(\"src.tasks.utils_typing\")\n",
                "\n",
                "sys.modules[\"src\"] = src\n",
                "sys.modules[\"src.tasks\"] = src_tasks\n",
                "sys.modules[\"src.tasks.utils_typing\"] = src_tasks_utils_typing\n",
                "\n",
                "# Define the base Entity class and dataclass\n",
                "@dataclass\n",
                "class Entity:\n",
                "    span: str = None\n",
                "\n",
                "src_tasks_utils_typing.Entity = Entity\n",
                "src_tasks_utils_typing.dataclass = dataclass"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Load Model with Quantization (Fits in 16GB free Colab GPU)\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "\n",
                "model_id = \"HiTZ/GoLLIE-7B\"\n",
                "\n",
                "# 4-bit quantization configuration\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16  # T4 supports float16\n",
                ")\n",
                "\n",
                "print(f\"Loading {model_id} in 4-bit mode...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "\n",
                "# We do NOT enforce use_flash_attention_2=True here data to T4 compatibility.\n",
                "# Transformers will automatically pick the best supported attention implementation.\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "print(\"Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Define Inference Helper Function\n",
                "def generate_response(prompt, model, tokenizer):\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=256,\n",
                "            do_sample=False, # Deterministic for extraction\n",
                "            pad_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "    \n",
                "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "\n",
                "print(\"Inference function ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Import your Guidelines\n",
                "# Make sure you have uploaded 'guidelines_fine_gollie.py' to Colab files!\n",
                "\n",
                "try:\n",
                "    import guidelines_fine_gollie as guidelines\n",
                "    print(\"Loaded guidelines:\", dir(guidelines))\n",
                "    print(\"Entities defined:\", [x.__name__ for x in guidelines.ENTITY_DEFINITIONS])\n",
                "except ImportError:\n",
                "    print(\"Error: guidelines_fine_gollie.py not found. Please upload it to the Files section.\")\n",
                "except Exception as e:\n",
                "    print(f\"Error loading guidelines: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Run Inference Example\n",
                "# Construct a prompt following GoLLIE format (simplification)\n",
                "# You typically need to prompt with the class definitions + input.\n",
                "\n",
                "# Example input text\n",
                "text_input = \"The Shawshank Redemption is a 1994 American drama film written by Frank Darabont.\"\n",
                "\n",
                "# Note: GoLLIE follows a specific prompt template. \n",
                "# The simplest way is to inspect `guidelines.ENTITY_DEFINITIONS` and construct the instruction.\n",
                "# Below is a placeholder for testing model liveness.\n",
                "\n",
                "prompt = f\"Extract entities from the following text based on the guidelines: \\n\\nText: {text_input}\\n\\nAnswer:\"\n",
                "\n",
                "response = generate_response(prompt, model, tokenizer)\n",
                "print(\"OUTPUT:\", response)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}